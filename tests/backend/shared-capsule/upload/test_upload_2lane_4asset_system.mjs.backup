/**
 * 2-Lane + 4-Asset Upload System Test
 *
 * This test reproduces the S3 2-lane + 4-asset system using ICP backend:
 * - Lane A: Upload original file to ICP blob storage
 * - Lane B: Process image derivatives (display, thumb, placeholder)
 * - Finalize: Create memory with all 4 asset types
 *
 * Uses functional approach to match frontend S3 system pattern.
 *
 * Reference Frontend Files:
 * - Main S3 Service: src/nextjs/src/lib/s3.ts
 * - 2-Lane + 4-Asset System: src/nextjs/src/services/upload/s3-with-processing.ts
 * - Image Processing: src/nextjs/src/services/upload/image-derivatives.ts
 * - Finalization: src/nextjs/src/services/upload/finalize.ts
 * - S3 Grants: src/nextjs/src/services/upload/s3-grant.ts
 * - Shared Utils: src/nextjs/src/services/upload/shared-utils.ts
 *
 * TODO: Import and reuse frontend functions where possible:
 * - processImageDerivativesPure() for real image processing
 * - Asset metadata structures and types
 * - Utility functions for file handling and validation
 */

import { Actor, HttpAgent } from "@dfinity/agent";
import { loadDfxIdentity, makeMainnetAgent } from "./ic-identity.js";
import crypto from "node:crypto";
import fs from "node:fs";
import path from "node:path";
import { idlFactory } from "./declarations/backend/backend.did.js";
import {
  validateFileSize,
  validateImageType,
  calculateFileHash,
  generateFileId,
  calculateDerivativeDimensions,
  calculateDerivativeSizes,
  createFileChunks,
  createProgressCallback,
  createAssetMetadata,
  createBlobReference,
  handleUploadError,
  validateUploadResponse,
  formatFileSize,
  formatUploadSpeed,
  formatDuration,
} from "./helpers.mjs";

// Test configuration
const TEST_NAME = "2-Lane + 4-Asset Upload System Test";
const TEST_IMAGE_PATH = "./assets/input/avocado_big_21mb.jpg";
// Constants - Aligned with backend configuration
const CHUNK_SIZE = 1_800_000; // 1.8MB - matches backend CHUNK_SIZE in types.rs
const INLINE_MAX = 32 * 1024; // 32KB - matches backend INLINE_MAX in types.rs

// Derivative asset storage strategy (from frontend S3 system):
// - Display: Blob storage + chunked upload (~100KB-2MB)
// - Thumb: Blob storage + chunked upload (~10KB-200KB)
// - Placeholder: Inline storage (~1KB-10KB, data URL in database)

// Global backend instance
let backend;

// Helper functions
function echoInfo(message) {
  console.log(`‚ÑπÔ∏è  ${message}`);
}

function echoPass(message) {
  console.log(`‚úÖ ${message}`);
}

function echoFail(message) {
  console.log(`‚ùå ${message}`);
}

function echoWarning(message) {
  console.log(`‚ö†Ô∏è  ${message}`);
}

// Real image processing (Node.js version of frontend logic)
async function processImageDerivativesPure(fileBuffer, mimeType) {
  const originalSize = fileBuffer.length;

  echoInfo(`üñºÔ∏è Processing derivatives for ${formatFileSize(originalSize)} file`);

  // Validate file type using helper
  validateImageType(mimeType);

  // Get derivative size limits from helper
  const sizeLimits = calculateDerivativeSizes(originalSize);

  // Calculate realistic dimensions
  const aspectRatio = 16 / 9;
  const originalWidth = Math.floor(Math.sqrt(originalSize / 3));
  const originalHeight = Math.floor(originalWidth / aspectRatio);

  // Calculate derivative dimensions using helper
  const displayDims = calculateDerivativeDimensions(
    originalWidth,
    originalHeight,
    sizeLimits.display.maxWidth,
    sizeLimits.display.maxHeight
  );
  const thumbDims = calculateDerivativeDimensions(
    originalWidth,
    originalHeight,
    sizeLimits.thumb.maxWidth,
    sizeLimits.thumb.maxHeight
  );

  // Create derivative buffers (simulation - in real implementation, use Sharp/Jimp)
  const displaySize = Math.min(sizeLimits.display.maxSize, Math.floor(originalSize * 0.1));
  const displayBuffer = Buffer.alloc(displaySize);
  fileBuffer.copy(displayBuffer, 0, 0, displaySize);

  const thumbSize = Math.min(sizeLimits.thumb.maxSize, Math.floor(originalSize * 0.05));
  const thumbBuffer = Buffer.alloc(thumbSize);
  fileBuffer.copy(thumbBuffer, 0, 0, thumbSize);

  const placeholderSize = Math.min(sizeLimits.placeholder.maxSize, 1024);
  const placeholderBuffer = Buffer.alloc(placeholderSize, 0x42);

  // Log precise sizes using helper
  echoInfo(`üìä Derivative sizes:`);
  echoInfo(`  Display: ${formatFileSize(displaySize)} (${displayDims.width}x${displayDims.height})`);
  echoInfo(`  Thumb: ${formatFileSize(thumbSize)} (${thumbDims.width}x${thumbDims.height})`);
  echoInfo(`  Placeholder: ${formatFileSize(placeholderSize)} (32x18)`);

  return {
    original: {
      buffer: fileBuffer,
      size: originalSize,
      width: originalWidth,
      height: originalHeight,
      mimeType: mimeType,
    },
    display: {
      buffer: displayBuffer,
      size: displaySize,
      width: displayDims.width,
      height: displayDims.height,
      mimeType: "image/webp",
    },
    thumb: {
      buffer: thumbBuffer,
      size: thumbSize,
      width: thumbDims.width,
      height: thumbDims.height,
      mimeType: "image/webp",
    },
    placeholder: {
      buffer: placeholderBuffer,
      size: placeholderSize,
      width: 32,
      height: 18,
      mimeType: "image/webp",
    },
  };
}

// Lane A: Upload original file to ICP (matches frontend uploadOriginalToS3)
async function uploadOriginalToICP(backend, fileBuffer, fileName) {
  const startTime = Date.now();

  echoInfo(`üì§ Uploading: ${fileName} (${formatFileSize(fileBuffer.length)})`);

  // Validate file size using helper
  validateFileSize(fileBuffer.length);

  // Create a new capsule for this test
  const capsuleResult = await backend.capsules_create([]);
  if ("Err" in capsuleResult) {
    throw new Error(`Capsule creation failed: ${JSON.stringify(capsuleResult.Err)}`);
  }
  const capsuleId = capsuleResult.Ok.id;

  // Calculate chunk count and create chunks using helpers
  const chunkCount = Math.ceil(fileBuffer.length / CHUNK_SIZE);
  const chunks = createFileChunks(fileBuffer, CHUNK_SIZE);
  const idempotencyKey = generateFileId("upload");

  // Begin upload session (no assetMetadata needed anymore)
  const beginResult = await backend.uploads_begin(capsuleId, chunkCount, idempotencyKey);

  // Handle different response formats
  let sessionId;
  if (typeof beginResult === "number" || typeof beginResult === "string") {
    // Direct response (number or string) - this is the current backend behavior
    sessionId = beginResult;
    echoInfo(`‚úÖ Upload session started: ${sessionId}`);
  } else if (beginResult && typeof beginResult === "object") {
    // Object response with Ok/Err structure
    try {
      validateUploadResponse(beginResult, ["Ok"]);
      sessionId = beginResult.Ok;
      echoInfo(`‚úÖ Upload session started: ${sessionId}`);
    } catch (error) {
      throw handleUploadError(error, "Upload begin");
    }
  } else {
    throw new Error(`Unexpected response format: ${typeof beginResult} - ${JSON.stringify(beginResult)}`);
  }

  // Upload file in chunks with progress tracking
  echoInfo(`üì¶ Uploading ${chunks.length} chunks (${formatFileSize(CHUNK_SIZE)} each)...`);

  const progressCallback = createProgressCallback(chunks.length, (progress, completed, total) => {
    if (completed % 5 === 0 || completed === total) {
      echoInfo(`  üìà ${progress}% (${completed}/${total} chunks)`);
    }
  });

  for (let i = 0; i < chunks.length; i++) {
    const putChunkResult = await backend.uploads_put_chunk(sessionId, i, chunks[i]);

    // Handle different response formats for put_chunk
    if (typeof putChunkResult === "object" && putChunkResult !== null) {
      try {
        validateUploadResponse(putChunkResult);
      } catch (error) {
        throw handleUploadError(error, `Put chunk ${i}`);
      }
    } else {
      // Direct response (success) - no validation needed
      echoInfo(`‚úÖ Chunk ${i} uploaded successfully`);
    }

    progressCallback(i);
  }

  // Calculate hash and total length for finish using helpers
  const hash = calculateFileHash(fileBuffer);
  const totalLen = BigInt(fileBuffer.length);

  // Finish upload
  const finishResult = await backend.uploads_finish(sessionId, Array.from(hash), totalLen);

  // Handle different response formats for finish
  let blobId;
  if (typeof finishResult === "string") {
    // Direct response - blob ID only (new format after refactoring)
    blobId = finishResult;
    echoInfo(`‚úÖ Upload finished successfully: blob_id=${blobId}`);
  } else if (finishResult && typeof finishResult === "object") {
    // Object response with Ok/Err structure
    try {
      validateUploadResponse(finishResult, ["Ok"]);
      const result = finishResult.Ok;
      if (result && typeof result === "object" && "blob_id" in result) {
        // New format: UploadFinishResult with blob_id only
        blobId = result.blob_id;
        echoInfo(`‚úÖ Upload finished successfully: blob_id=${blobId}`);
      } else {
        // Legacy format: direct string
        blobId = result;
        echoInfo(`‚úÖ Upload finished successfully: blob_id=${blobId}`);
      }
    } catch (error) {
      throw handleUploadError(error, "Upload finish");
    }
  } else {
    throw new Error(`Unexpected finish response format: ${typeof finishResult} - ${JSON.stringify(finishResult)}`);
  }

  const duration = Date.now() - startTime;
  const uploadSpeed = formatUploadSpeed(fileBuffer.length, duration);

  echoInfo(
    `‚úÖ Upload completed: ${fileName} (${formatFileSize(fileBuffer.length)}) in ${formatDuration(
      duration
    )} (${uploadSpeed})`
  );

  return blobId;
}

// Lane B: Process image derivatives (matches frontend processImageDerivativesPure)
async function processImageDerivativesToICP(backend, fileBuffer, mimeType) {
  const laneBStartTime = Date.now();
  echoInfo(`üñºÔ∏è Starting Lane B: Processing derivatives`);

  const processedAssets = await processImageDerivativesPure(fileBuffer, mimeType);

  // Upload each derivative to ICP
  const results = {};
  const uploadPromises = [];

  if (processedAssets.display) {
    echoInfo(`üì§ Uploading display derivative...`);
    uploadPromises.push(
      uploadOriginalToICP(backend, processedAssets.display.buffer, "display").then((blobId) => {
        results.display = blobId;
      })
    );
  }

  if (processedAssets.thumb) {
    echoInfo(`üì§ Uploading thumb derivative...`);
    uploadPromises.push(
      uploadOriginalToICP(backend, processedAssets.thumb.buffer, "thumb").then((blobId) => {
        results.thumb = blobId;
      })
    );
  }

  if (processedAssets.placeholder) {
    echoInfo(`üì§ Uploading placeholder derivative...`);
    uploadPromises.push(
      uploadOriginalToICP(backend, processedAssets.placeholder.buffer, "placeholder").then((blobId) => {
        results.placeholder = blobId;
      })
    );
  }

  // Wait for all uploads to complete
  await Promise.all(uploadPromises);

  const laneBDuration = Date.now() - laneBStartTime;
  const totalAssets = Object.keys(results).length;
  echoInfo(`‚úÖ Lane B completed: ${totalAssets} derivatives uploaded in ${laneBDuration}ms`);

  return results;
}

// Finalize all assets (matches frontend finalizeAllAssets)
async function finalizeAllAssets(backend, originalBlobId, results, fileName) {
  // Create a new capsule for this test
  const capsuleResult = await backend.capsules_create([]);
  if ("Err" in capsuleResult) {
    throw new Error(`Capsule creation failed: ${JSON.stringify(capsuleResult.Err)}`);
  }
  const capsuleId = capsuleResult.Ok.id;

  // Create memory metadata
  const memoryMetadata = {
    title: [fileName], // opt text - wrapped in array for Some(value)
    description: ["2-Lane + 4-Asset System Test Memory"], // opt text
    tags: ["test", "2lane-4asset"],
    created_at: BigInt(Date.now() * 1000000),
    updated_at: BigInt(Date.now() * 1000000),
    date_of_memory: [],
    memory_type: { Image: null },
    content_type: "image/jpeg",
    people_in_memory: [],
    database_storage_edges: [],
    created_by: [],
    parent_folder_id: [],
    deleted_at: [],
    file_created_at: [],
    location: [],
    memory_notes: [],
    uploaded_at: BigInt(Date.now() * 1000000),
  };

  // Create asset metadata for the original blob
  const assetMetadata = {
    Image: {
      dpi: [],
      color_space: [],
      base: {
        url: [],
        height: [],
        updated_at: BigInt(Date.now() * 1000000),
        asset_type: { Original: null },
        sha256: [],
        name: fileName,
        storage_key: [],
        tags: ["test", "2lane-4asset", "original"],
        processing_error: [],
        mime_type: "image/jpeg",
        description: [],
        created_at: BigInt(Date.now() * 1000000),
        deleted_at: [],
        bytes: BigInt(0), // Will be updated with actual size
        asset_location: [],
        width: [],
        processing_status: [],
        bucket: [],
      },
      exif_data: [],
      compression_ratio: [],
      orientation: [],
    },
  };

  // Create asset metadata for derivatives
  const derivativeAssetMetadata = {
    Image: {
      dpi: [],
      color_space: [],
      base: {
        url: [],
        height: [],
        updated_at: BigInt(Date.now() * 1000000),
        asset_type: { Derivative: null },
        sha256: [],
        name: "derivative",
        storage_key: [],
        tags: ["test", "2lane-4asset", "derivative"],
        processing_error: [],
        mime_type: "image/jpeg",
        description: [],
        created_at: BigInt(Date.now() * 1000000),
        deleted_at: [],
        bytes: BigInt(0),
        asset_location: [],
        width: [],
        processing_status: [],
        bucket: [],
      },
      exif_data: [],
      compression_ratio: [],
      orientation: [],
    },
  };

  // Create memory with all 4 assets (original + 3 derivatives)
  const allAssets = [
    { blob_id: originalBlobId, metadata: assetMetadata },
    { blob_id: results.display, metadata: derivativeAssetMetadata },
    { blob_id: results.thumb, metadata: derivativeAssetMetadata },
    { blob_id: results.placeholder, metadata: derivativeAssetMetadata },
  ];

  echoInfo(`üìù Creating memory with ${allAssets.length} assets...`);
  echoInfo(`  Original: ${originalBlobId}`);
  echoInfo(`  Display: ${results.display}`);
  echoInfo(`  Thumb: ${results.thumb}`);
  echoInfo(`  Placeholder: ${results.placeholder}`);

  const memoryResult = await backend.memories_create_with_internal_blobs(
    capsuleId, // text - capsule ID
    memoryMetadata, // MemoryMetadata
    allAssets, // Vec<InternalBlobAssetInput> - all 4 assets
    `memory-${Date.now()}` // text - idempotency key
  );

  if ("Err" in memoryResult) {
    echoFail(`Memory creation failed: ${JSON.stringify(memoryResult.Err)}`);
    throw new Error(`Memory creation failed: ${JSON.stringify(memoryResult.Err)}`);
  }

  const memoryId = memoryResult.Ok;

  return {
    memoryId,
    originalBlobId,
    processedAssets: results,
  };
}

// Main upload function (matches frontend uploadToS3WithProcessing)
async function uploadToICPWithProcessing(backend, fileBuffer, fileName, mimeType) {
  try {
    // Start both lanes simultaneously
    const laneAPromise = uploadOriginalToICP(backend, fileBuffer, fileName);
    const laneBPromise = processImageDerivativesToICP(backend, fileBuffer, mimeType);

    // Wait for both lanes to complete
    const [laneAResult, laneBResult] = await Promise.allSettled([laneAPromise, laneBPromise]);

    // Finalize all assets
    if (laneAResult.status === "fulfilled" && laneBResult.status === "fulfilled") {
      const finalResult = await finalizeAllAssets(backend, laneAResult.value, laneBResult.value, fileName);
      return finalResult;
    } else {
      throw new Error(`Lane failed: A=${laneAResult.status}, B=${laneBResult.status}`);
    }
  } catch (error) {
    throw error;
  }
}

// Test functions
async function testLaneAOriginalUpload() {
  const fileBuffer = fs.readFileSync(TEST_IMAGE_PATH);
  const fileName = path.basename(TEST_IMAGE_PATH);

  const blobId = await uploadOriginalToICP(backend, fileBuffer, fileName);

  // Verify blob was created
  const blobMeta = await backend.blob_get_meta(blobId);
  if ("Err" in blobMeta) {
    throw new Error(`Failed to get blob meta: ${JSON.stringify(blobMeta.Err)}`);
  }

  return blobMeta.Ok.size === BigInt(fileBuffer.length);
}

async function testLaneBImageProcessing() {
  const fileBuffer = fs.readFileSync(TEST_IMAGE_PATH);

  const processedAssets = await processImageDerivativesPure(fileBuffer, "image/jpeg");

  // Verify all derivatives were created
  return processedAssets.original && processedAssets.display && processedAssets.thumb && processedAssets.placeholder;
}

async function testParallelLanes() {
  const fileBuffer = fs.readFileSync(TEST_IMAGE_PATH);
  const fileName = path.basename(TEST_IMAGE_PATH);

  // Start both lanes simultaneously
  const laneAPromise = uploadOriginalToICP(backend, fileBuffer, fileName);
  const laneBPromise = processImageDerivativesToICP(backend, fileBuffer, "image/jpeg");

  // Wait for both lanes to complete
  const [laneAResult, laneBResult] = await Promise.allSettled([laneAPromise, laneBPromise]);

  return laneAResult.status === "fulfilled" && laneBResult.status === "fulfilled";
}

async function testCompleteSystem() {
  const fileBuffer = fs.readFileSync(TEST_IMAGE_PATH);
  const fileName = path.basename(TEST_IMAGE_PATH);

  const result = await uploadToICPWithProcessing(backend, fileBuffer, fileName, "image/jpeg");

  // Verify all assets were created
  const hasOriginal = result.originalBlobId !== null;
  const hasDisplay = result.processedAssets.display !== null;
  const hasThumb = result.processedAssets.thumb !== null;
  const hasPlaceholder = result.processedAssets.placeholder !== null;

  return hasOriginal && hasDisplay && hasThumb && hasPlaceholder;
}

async function testAssetRetrieval() {
  const fileBuffer = fs.readFileSync(TEST_IMAGE_PATH);
  const fileName = path.basename(TEST_IMAGE_PATH);

  const result = await uploadToICPWithProcessing(backend, fileBuffer, fileName, "image/jpeg");

  // Test retrieval of original asset
  const originalMeta = await backend.blob_get_meta(result.originalBlobId);
  if ("Err" in originalMeta) {
    throw new Error(`Failed to get original meta: ${JSON.stringify(originalMeta.Err)}`);
  }

  // Test retrieval of display asset
  if (result.processedAssets.display) {
    const displayMeta = await backend.blob_get_meta(result.processedAssets.display);
    if ("Err" in displayMeta) {
      throw new Error(`Failed to get display meta: ${JSON.stringify(displayMeta.Err)}`);
    }
  }

  return true;
}

async function testFullDeletionWorkflow() {
  const fileBuffer = fs.readFileSync(TEST_IMAGE_PATH);
  const fileName = path.basename(TEST_IMAGE_PATH);

  echoInfo(`üß™ Testing full deletion workflow with ${fileName}`);

  // Step 1: Create memory with all assets
  const result = await uploadToICPWithProcessing(backend, fileBuffer, fileName, "image/jpeg");
  echoInfo(`‚úÖ Created memory: ${result.memoryId}`);
  echoInfo(
    `‚úÖ Created assets: original=${result.originalBlobId}, display=${result.processedAssets.display}, thumb=${result.processedAssets.thumb}, placeholder=${result.processedAssets.placeholder}`
  );

  // Step 2: Verify all assets exist before deletion
  const allBlobIds = [
    result.originalBlobId,
    result.processedAssets.display,
    result.processedAssets.thumb,
    result.processedAssets.placeholder,
  ].filter(Boolean);

  echoInfo(`üîç Verifying ${allBlobIds.length} assets exist before deletion...`);
  for (const blobId of allBlobIds) {
    const meta = await backend.blob_get_meta(blobId);
    if ("Err" in meta) {
      throw new Error(`Asset ${blobId} not found before deletion: ${JSON.stringify(meta.Err)}`);
    }
    echoInfo(`  ‚úÖ Asset ${blobId} exists (${meta.Ok.size} bytes)`);
  }

  // Step 3: Verify memory exists
  const memoryRead = await backend.memories_read(result.memoryId);
  if ("Err" in memoryRead) {
    throw new Error(`Memory not found before deletion: ${JSON.stringify(memoryRead.Err)}`);
  }
  echoInfo(`‚úÖ Memory exists with ${memoryRead.Ok.blob_internal_assets.length} internal assets`);

  // Step 4: Delete memory with assets (full deletion)
  echoInfo(`üóëÔ∏è Deleting memory with assets (delete_assets: true)...`);
  const deleteResult = await backend.memories_delete(result.memoryId, true);
  if ("Err" in deleteResult) {
    throw new Error(`Memory deletion failed: ${JSON.stringify(deleteResult.Err)}`);
  }
  echoInfo(`‚úÖ Memory deleted successfully`);

  // Step 5: Verify memory is gone
  const memoryReadAfter = await backend.memories_read(result.memoryId);
  if ("Ok" in memoryReadAfter) {
    throw new Error(`Memory still exists after deletion: ${result.memoryId}`);
  }
  echoInfo(`‚úÖ Memory confirmed deleted`);

  // Step 6: Verify all assets are gone
  echoInfo(`üîç Verifying all assets are deleted...`);
  for (const blobId of allBlobIds) {
    const meta = await backend.blob_get_meta(blobId);
    if ("Ok" in meta) {
      throw new Error(`Asset ${blobId} still exists after deletion`);
    }
    if ("Err" in meta && "NotFound" in meta.Err) {
      echoInfo(`  ‚úÖ Asset ${blobId} confirmed deleted`);
    } else {
      throw new Error(`Unexpected error for asset ${blobId}: ${JSON.stringify(meta.Err)}`);
    }
  }

  echoInfo(`‚úÖ Full deletion workflow completed successfully - memory and all assets deleted`);
  return true;
}

async function testSelectiveDeletionWorkflow() {
  const fileBuffer = fs.readFileSync(TEST_IMAGE_PATH);
  const fileName = path.basename(TEST_IMAGE_PATH);

  echoInfo(`üß™ Testing selective deletion workflow with ${fileName}`);

  // Step 1: Create memory with all assets
  const result = await uploadToICPWithProcessing(backend, fileBuffer, fileName, "image/jpeg");
  echoInfo(`‚úÖ Created memory: ${result.memoryId}`);

  // Step 2: Verify all assets exist before deletion
  const allBlobIds = [
    result.originalBlobId,
    result.processedAssets.display,
    result.processedAssets.thumb,
    result.processedAssets.placeholder,
  ].filter(Boolean);

  echoInfo(`üîç Verifying ${allBlobIds.length} assets exist before selective deletion...`);
  for (const blobId of allBlobIds) {
    const meta = await backend.blob_get_meta(blobId);
    if ("Err" in meta) {
      throw new Error(`Asset ${blobId} not found before deletion: ${JSON.stringify(meta.Err)}`);
    }
    echoInfo(`  ‚úÖ Asset ${blobId} exists (${meta.Ok.size} bytes)`);
  }

  // Step 3: Delete memory without assets (metadata-only deletion)
  echoInfo(`üóëÔ∏è Deleting memory without assets (delete_assets: false)...`);
  const deleteResult = await backend.memories_delete(result.memoryId, false);
  if ("Err" in deleteResult) {
    throw new Error(`Memory deletion failed: ${JSON.stringify(deleteResult.Err)}`);
  }
  echoInfo(`‚úÖ Memory metadata deleted successfully`);

  // Step 4: Verify memory is gone
  const memoryReadAfter = await backend.memories_read(result.memoryId);
  if ("Ok" in memoryReadAfter) {
    throw new Error(`Memory still exists after deletion: ${result.memoryId}`);
  }
  echoInfo(`‚úÖ Memory confirmed deleted`);

  // Step 5: Verify all assets are preserved
  echoInfo(`üîç Verifying all assets are preserved...`);
  for (const blobId of allBlobIds) {
    const meta = await backend.blob_get_meta(blobId);
    if ("Err" in meta) {
      throw new Error(`Asset ${blobId} was deleted when it should be preserved: ${JSON.stringify(meta.Err)}`);
    }
    echoInfo(`  ‚úÖ Asset ${blobId} preserved (${meta.Ok.size} bytes)`);
  }

  echoInfo(`‚úÖ Selective deletion workflow completed successfully - memory deleted, assets preserved`);
  return true;
}

// Focused unit test for delete function with multiple assets
async function testDeleteFunctionUnit() {
  echoInfo(`üß™ Testing delete function unit test with multiple assets`);

  // Step 1: Create a memory with multiple internal blob assets
  echoInfo(`üì§ Creating memory with 4 internal blob assets...`);

  // Upload original file
  const filePath = "assets/input/avocado_big_21mb.jpg";
  const fileBuffer = readFileSync(filePath);
  const fileSize = fileBuffer.length;

  // Create upload session
  const beginResult = await backend.uploads_begin("test-delete-unit", fileSize);
  const sessionId = beginResult.Ok;

  // Upload in chunks
  const chunks = createChunks(fileBuffer, CHUNK_SIZE);
  for (let i = 0; i < chunks.length; i++) {
    await backend.uploads_put_chunk(sessionId, i, chunks[i]);
  }

  // Finish upload
  const hash = calculateFileHash(fileBuffer);
  const totalLen = BigInt(fileBuffer.length);
  const originalBlobId = await backend.uploads_finish(sessionId, Array.from(hash), totalLen);

  // Create 3 additional blob assets (simulating derivatives)
  const derivativeBlobIds = [];
  for (let i = 0; i < 3; i++) {
    const derivativeSessionId = (await backend.uploads_begin(`derivative-${i}`, 1000)).Ok;
    const derivativeChunk = new Uint8Array(1000).fill(i + 1);
    await backend.uploads_put_chunk(derivativeSessionId, 0, derivativeChunk);
    const derivativeHash = Array.from(crypto.createHash("sha256").update(derivativeChunk).digest());
    const derivativeBlobId = await backend.uploads_finish(derivativeSessionId, derivativeHash, BigInt(1000));
    derivativeBlobIds.push(derivativeBlobId);
  }

  // Create memory with all 4 blob assets
  const memoryMetadata = {
    title: ["Delete Unit Test"],
    description: ["Testing delete function with multiple assets"],
    tags: [],
    created_at: BigInt(Date.now() * 1000000),
    updated_at: BigInt(Date.now() * 1000000),
  };

  const internalBlobAssets = [
    {
      blob_id: originalBlobId,
      metadata: {
        Image: {
          base: {
            name: "original",
            description: ["Original file"],
            tags: [],
            asset_type: { Original: null },
            bytes: BigInt(fileSize),
            mime_type: "image/jpeg",
            sha256: null,
            width: null,
            height: null,
            url: null,
            storage_key: null,
            bucket: null,
            asset_location: null,
            processing_status: null,
            processing_error: null,
            created_at: BigInt(Date.now() * 1000000),
            updated_at: BigInt(Date.now() * 1000000),
            deleted_at: null,
          },
        },
      },
    },
    ...derivativeBlobIds.map((blobId, i) => ({
      blob_id: blobId,
      metadata: {
        Image: {
          base: {
            name: `derivative-${i}`,
            description: [`Derivative ${i}`],
            tags: [],
            asset_type: { Derivative: null },
            bytes: BigInt(1000),
            mime_type: "image/jpeg",
            sha256: null,
            width: null,
            height: null,
            url: null,
            storage_key: null,
            bucket: null,
            asset_location: null,
            processing_status: null,
            processing_error: null,
            created_at: BigInt(Date.now() * 1000000),
            updated_at: BigInt(Date.now() * 1000000),
            deleted_at: null,
          },
        },
      },
    })),
  ];

  const createResult = await backend.memories_create_with_internal_blobs(
    "capsule_1759713283267064000",
    memoryMetadata,
    internalBlobAssets,
    `delete-unit-test-${Date.now()}`
  );

  if ("Err" in createResult) {
    throw new Error(`Memory creation failed: ${JSON.stringify(createResult.Err)}`);
  }

  const memoryId = createResult.Ok;
  echoInfo(`‚úÖ Created memory: ${memoryId}`);

  const allBlobIds = [originalBlobId, ...derivativeBlobIds];
  echoInfo(`‚úÖ Created ${allBlobIds.length} assets: ${allBlobIds.join(", ")}`);

  // Step 2: Verify all assets exist
  echoInfo(`üîç Verifying all ${allBlobIds.length} assets exist before deletion...`);
  for (const blobId of allBlobIds) {
    const meta = await backend.blob_get_meta(blobId);
    if ("Err" in meta) {
      throw new Error(`Asset ${blobId} not found before deletion: ${JSON.stringify(meta.Err)}`);
    }
    echoInfo(`  ‚úÖ Asset ${blobId} exists (${meta.Ok.size} bytes)`);
  }

  // Step 3: Test full deletion (delete_assets: true)
  echoInfo(`üóëÔ∏è Testing full deletion (delete_assets: true)...`);
  const deleteResult = await backend.memories_delete(memoryId, true);
  if ("Err" in deleteResult) {
    throw new Error(`Memory deletion failed: ${JSON.stringify(deleteResult.Err)}`);
  }
  echoInfo(`‚úÖ Memory deleted successfully`);

  // Step 4: Verify memory is gone
  const memoryReadAfter = await backend.memories_read(memoryId);
  if ("Ok" in memoryReadAfter) {
    throw new Error(`Memory still exists after deletion: ${memoryId}`);
  }
  echoInfo(`‚úÖ Memory confirmed deleted`);

  // Step 5: Verify ALL assets are deleted
  echoInfo(`üîç Verifying all ${allBlobIds.length} assets are deleted...`);
  for (const blobId of allBlobIds) {
    const meta = await backend.blob_get_meta(blobId);
    if ("Ok" in meta) {
      throw new Error(`Asset ${blobId} still exists after deletion - should be deleted!`);
    }
    if ("Err" in meta && "NotFound" in meta.Err) {
      echoInfo(`  ‚úÖ Asset ${blobId} confirmed deleted`);
    } else {
      throw new Error(`Asset ${blobId} deletion check failed: ${JSON.stringify(meta)}`);
    }
  }

  echoInfo(`‚úÖ Delete function unit test completed successfully - all ${allBlobIds.length} assets deleted`);
  return true;
}

// Main test runner
async function main() {
  echoInfo(`Starting ${TEST_NAME}`);

  // Parse command line arguments
  const args = process.argv.slice(2);
  const backendCanisterId = args[0];
  const network = args[1] || "local"; // Default to local network
  const testFilter = args[2]; // Optional test filter

  if (!backendCanisterId) {
    echoFail("Usage: node test_upload_2lane_4asset_system.mjs <CANISTER_ID> [mainnet|local] [test_name]");
    echoFail("Example: node test_upload_2lane_4asset_system.mjs uxrrr-q7777-77774-qaaaq-cai local");
    echoFail("Example: node test_upload_2lane_4asset_system.mjs uxrrr-q7777-77774-qaaaq-cai mainnet");
    echoFail(
      "Example: node test_upload_2lane_4asset_system.mjs uxrrr-q7777-77774-qaaaq-cai local 'Complete 2-Lane + 4-Asset System'"
    );
    echoFail("");
    echoFail("Available tests:");
    echoFail("  - 'Lane A: Original Upload'");
    echoFail("  - 'Lane B: Image Processing'");
    echoFail("  - 'Parallel Lanes Execution'");
    echoFail("  - 'Complete 2-Lane + 4-Asset System'");
    echoFail("  - 'Asset Retrieval'");
    echoFail("  - 'Full Deletion Workflow'");
    echoFail("  - 'Selective Deletion Workflow'");
    process.exit(1);
  }

  // Setup agent and backend based on network
  const identity = loadDfxIdentity();
  let agent;

  if (network === "mainnet") {
    echoInfo(`üåê Connecting to mainnet (ic0.app)`);
    agent = makeMainnetAgent(identity);
  } else {
    echoInfo(`üè† Connecting to local network (127.0.0.1:4943)`);
    agent = new HttpAgent({
      host: "http://127.0.0.1:4943",
      identity,
      fetch: (await import("node-fetch")).default,
    });
  }

  await agent.fetchRootKey();

  backend = Actor.createActor(idlFactory, {
    agent,
    canisterId: backendCanisterId,
  });

  // Run tests
  const allTests = [
    { name: "Lane A: Original Upload", fn: testLaneAOriginalUpload },
    { name: "Lane B: Image Processing", fn: testLaneBImageProcessing },
    { name: "Parallel Lanes Execution", fn: testParallelLanes },
    { name: "Complete 2-Lane + 4-Asset System", fn: testCompleteSystem },
    { name: "Asset Retrieval", fn: testAssetRetrieval },
    { name: "Full Deletion Workflow", fn: testFullDeletionWorkflow },
    { name: "Selective Deletion Workflow", fn: testSelectiveDeletionWorkflow },
  ];

  // Filter tests if test name is provided
  const tests = testFilter ? allTests.filter((test) => test.name === testFilter) : allTests;

  if (testFilter && tests.length === 0) {
    echoFail(`Test not found: "${testFilter}"`);
    echoFail("Available tests:");
    allTests.forEach((test) => echoFail(`  - "${test.name}"`));
    process.exit(1);
  }

  if (testFilter) {
    echoInfo(`Running single test: "${testFilter}"`);
  } else {
    echoInfo(`Running all ${tests.length} tests`);
  }

  let passed = 0;
  let failed = 0;

  for (const test of tests) {
    try {
      echoInfo(`Running: ${test.name}`);
      const result = await test.fn();
      if (result) {
        echoPass(test.name);
        passed++;
      } else {
        echoFail(test.name);
        failed++;
      }
    } catch (error) {
      echoFail(`${test.name}: ${error.message}`);
      failed++;
    }
  }

  // Summary
  echoInfo(`\n${TEST_NAME} Summary:`);
  echoInfo(`Total tests: ${tests.length}`);
  echoInfo(`Passed: ${passed}`);
  echoInfo(`Failed: ${failed}`);

  if (failed > 0) {
    echoFail("Some tests failed! ‚ùå");
    process.exit(1);
  } else {
    echoPass("All tests passed! ‚úÖ");
  }
}

// Run the test
main().catch((error) => {
  echoFail(`Test execution failed: ${error.message}`);
  process.exit(1);
});
