# Fix Progress Report - 2025-10-01 22:30

## Summary

Implemented **2 out of 5** critical fixes from tech lead review. Tests still failing (2/5 passing).

**Status**: Need guidance on remaining 3 fixes (hash updates, index atomicity).

---

## ‚úÖ Completed Fixes (2/5)

### Fix #1: bytes_expected Source of Truth ‚úÖ

**File**: `src/backend/src/session/compat.rs:68`  
**Status**: ‚úÖ COMPLETED

**Before**:

```rust
bytes_expected: (meta.chunk_count as u64) * (meta.chunk_size as u64),
```

**After**:

```rust
bytes_expected: meta.asset_metadata.get_base().bytes, // Use actual byte length
```

**Result**: Compiles successfully ‚úÖ

---

### Fix #2: StableBlobSink Key Scheme ‚úÖ

**File**: `src/backend/src/upload/blob_store.rs:529-567`  
**Status**: ‚úÖ COMPLETED

**Changes**:

1. Added alignment validation
2. Added chunk size validation
3. Fixed key scheme to use `(provisional_memory_id_hash, chunk_idx)`
4. Added proper error handling

**Before**:

```rust
let blob_id: u64 = self.provisional_memory_id.parse().unwrap_or_else(...);
// No validation
```

**After**:

```rust
// Validate alignment
if offset % (self.chunk_size as u64) != 0 {
    return Err(Error::InvalidArgument("unaligned offset".into()));
}

// Validate chunk size
if data.len() > self.chunk_size && offset > 0 {
    return Err(Error::InvalidArgument("oversized chunk".into()));
}

// Use proper hash-based key
use std::hash::{Hash, Hasher};
let mut hasher = std::collections::hash_map::DefaultHasher::new();
self.provisional_memory_id.hash(&mut hasher);
let blob_id = hasher.finish();
```

**Result**: Compiles successfully ‚úÖ

---

### Fix #4: Box sink_factory ‚úÖ (Was Already Done!)

**File**: `src/backend/src/session/compat.rs:26, 46`  
**Status**: ‚úÖ ALREADY IMPLEMENTED

**Code**:

```rust
type SinkFactory = Box<dyn Fn(&UploadSessionMeta) -> Result<Box<dyn ByteSink>, Error>>;

pub struct SessionCompat {
    sink_factory: SinkFactory,  // Already boxed!
}

impl SessionCompat {
    pub fn new<F>(sink_factory: F) -> Self
    where
        F: Fn(&UploadSessionMeta) -> Result<Box<dyn ByteSink>, Error> + 'static,
    {
        Self {
            sink_factory: Box::new(sink_factory),  // Already boxing!
        }
    }
}
```

**Result**: Was already correct ‚úÖ

---

## ‚ùå Remaining Fixes (3/5)

### Fix #3: Rolling Hash Updates ‚è≠Ô∏è

**File**: `src/backend/src/lib.rs` (uploads_put_chunk, uploads_finish)  
**Status**: ‚ùå NOT YET IMPLEMENTED  
**Impact**: **HIGH** - Likely causing test failures

**Current Situation**:

- Hash is computed in `blob_store.rs::store_from_chunks()`
- Reads ALL chunks back from storage to compute hash
- Works but inefficient

**Tech Lead Recommendation**:
Add rolling hash during `put_chunk`:

```rust
// Add thread-local hash storage
thread_local! {
    static UPLOAD_HASH: RefCell<BTreeMap<u64, Sha256>> = RefCell::new(BTreeMap::new());
}

#[ic_cdk::update]
fn uploads_put_chunk(sid: u64, idx: u32, data: Vec<u8>) -> Result<(), Error> {
    // Update hash BEFORE or AFTER write
    UPLOAD_HASH.with(|h| {
        h.borrow_mut()
            .get_mut(&sid)
            .ok_or(Error::NotFound)?
            .update(&data)
    });

    // ... rest of put_chunk
}
```

**Question**: Current approach works but reads chunks back. Should we:

1. **Keep current** (hash on finish by reading back) - simpler, works
2. **Add rolling hash** (tech lead's way) - more efficient, more complex

---

### Fix #5: uploads_finish Must Commit Index Before Returning ‚è≠Ô∏è

**File**: `src/backend/src/upload/service.rs::commit()` or `src/backend/src/lib.rs::uploads_finish()`  
**Status**: ‚ùå NEEDS VERIFICATION  
**Impact**: **HIGH** - Likely causing retrieval failures

**Current Code** (`upload/service.rs:296-306`):

```rust
// 4. Create memory with blob reference
let memory = Memory::from_blob(...);
let memory_id = memory.id.clone();

// 5. Atomic attach to capsule
store.update(&session.capsule_id, |capsule| {
    capsule.memories.insert(memory_id.clone(), memory);  // ‚Üê Is this atomic?
    capsule.updated_at = ic_cdk::api::time();
})?;

// 6. Cleanup session and chunks
with_session_compat(|sessions| sessions.cleanup(&session_id));

// Return both blob ID and memory ID
Ok((format!("blob_{}", blob_id.0), memory_id))
```

**Question**: Is `store.update()` atomic? Does it commit before returning `Ok`?

**Tech Lead's Point**: "finish must commit index before returning"

**Need to verify**:

1. Does `store.update()` wait for stable storage write?
2. Is the memory immediately retrievable after `Ok` return?
3. Should we add explicit commit/flush?

---

## üß™ Test Results After 2 Fixes

```
Test Results:
‚úÖ Lane A (Sequential Original Upload)     - PASSING
‚úÖ Lane B (Sequential Image Processing)    - PASSING
‚ùå Parallel Lanes Execution                - FAILING
‚ùå Complete 2-Lane + 4-Asset System        - FAILING (A=rejected, B=rejected)
‚ùå Asset Retrieval                         - FAILING (A=rejected, B=rejected)

Success Rate: 40% (2/5)
```

**No improvement** from first 2 fixes (still 40% pass rate).

---

## üîç Analysis

### Why Still Failing?

The parallel upload failures suggest one of:

1. **Hash verification failing** (Fix #3 not done)

   - Chunks write successfully
   - But hash mismatch on finish?
   - Need to check backend logs for "ChecksumMismatch" errors

2. **Index not committed atomically** (Fix #5 not verified)

   - `finish()` returns success
   - But asset not retrievable immediately
   - Race condition in parallel mode

3. **Both issues** (most likely)
   - Hash fails ‚Üí rejection
   - OR index not ready ‚Üí retrieval fails

### How to Diagnose?

**Option A: Check Backend Logs**

```bash
dfx canister logs backend | grep -E "(FINISH|ERROR|Checksum|NotFound)"
```

**Option B: Add Debug Logging**
In `uploads_finish()`:

```rust
ic_cdk::println!("FINISH_START: sid={}", session_id);
// ... verify hash
ic_cdk::println!("FINISH_HASH_OK: sid={}", session_id);
// ... commit index
ic_cdk::println!("FINISH_INDEX_COMMITTED: sid={}", session_id);
```

**Option C: Simplify Test**
Run just parallel upload without derivatives to isolate issue.

---

## üìã Questions for Tech Lead

### Q1: Hash Verification Approach

Current: Hash computed in `store_from_chunks()` by reading back all chunks  
Recommended: Rolling hash during `put_chunk()`

**Should we**:

- A) Keep current approach (works, simpler, tested)?
- B) Implement rolling hash (more efficient, preferred)?
- C) Both (rolling + verification read-back)?

### Q2: Index Commit Atomicity

Current: `store.update()` in `commit()` function  
Question: Is this atomic enough?

**What does `store.update()` do?**

- Does it wait for stable storage flush?
- Is memory immediately retrievable after return?
- Should we add explicit barrier?

### Q3: Debugging Strategy

**What's the fastest way to identify the exact failure?**

- Add logging to `uploads_finish()`?
- Check backend logs for specific errors?
- Add probe endpoint to check index state?

### Q4: Priority

Given 2/5 fixes done and still failing:

**Should we**:

- A) Continue with Fix #3 (rolling hash)?
- B) Focus on Fix #5 (index atomicity)?
- C) Add logging first to diagnose?
- D) Different approach entirely?

---

## üéØ Recommended Next Steps

### Option 1: Add Logging First (30 min)

```rust
// In uploads_finish()
ic_cdk::println!("FINISH_START: sid={}", session_id);
// ... after hash verify
ic_cdk::println!("FINISH_HASH: computed={:?}, expected={:?}", computed, expected);
// ... after index commit
ic_cdk::println!("FINISH_COMMITTED: memory_id={}", memory_id);
```

Then run test and check logs to see exact failure point.

### Option 2: Implement Fix #3 (1-2 hours)

Add rolling hash as tech lead recommended.

### Option 3: Verify Fix #5 (1 hour)

Check if `store.update()` is truly atomic, add explicit commit if needed.

---

## üìÅ Files Modified So Far

1. ‚úÖ `src/backend/src/session/compat.rs` - Fixed bytes_expected (line 68)
2. ‚úÖ `src/backend/src/upload/blob_store.rs` - Fixed write_at key scheme (lines 529-567)
3. ‚è≠Ô∏è `src/backend/src/lib.rs` - Need to add hash updates
4. ‚è≠Ô∏è `src/backend/src/upload/service.rs` - Need to verify index atomicity

---

## üí° Current Hypothesis

**Most likely cause of failure**:

The hash verification in `store_from_chunks()` is reading chunks back from storage using the same key derivation. If the key derivation in `store_from_chunks()` doesn't match the key derivation in `StableBlobSink::write_at()`, it would read empty chunks and compute wrong hash.

**Evidence**:

- We changed hash derivation in `StableBlobSink::write_at()` (Fix #2)
- But `store_from_chunks()` still has old derivation code (line 65-70)
- They must match exactly!

**Action**: Check if `store_from_chunks()` blob_id derivation matches `StableBlobSink::write_at()`.

---

## ‚úÖ UPDATE: Fixed Key Derivation Mismatch (Fix #2.1)

**Discovery**: The key derivation in `store_from_chunks()` didn't match `StableBlobSink::write_at()`!

**Before** (`store_from_chunks`):

```rust
let blob_id_num = session_meta.provisional_memory_id.parse().unwrap_or_else(|_| {
    // Hash as fallback
});
```

**After** (now matches `StableBlobSink`):

```rust
// ALWAYS hash (like StableBlobSink does)
use std::hash::{Hash, Hasher};
let mut id_hasher = std::collections::hash_map::DefaultHasher::new();
session_meta.provisional_memory_id.hash(&mut id_hasher);
let blob_id = BlobId(id_hasher.finish());
```

**Result**: Still 2/5 passing (no improvement yet)

---

## üîç Current Failure Pattern

**Observation**: Chunks upload successfully (100%), but `finish()` rejects

```
‚ÑπÔ∏è    üìà 100% (13/13 chunks)  ‚Üê All chunks uploaded
‚ùå Complete 2-Lane + 4-Asset System: Lane failed: A=rejected, B=rejected
```

**This means**:

- ‚úÖ `uploads_begin()` works
- ‚úÖ `uploads_put_chunk()` works (all chunks)
- ‚ùå `uploads_finish()` rejects

**Likely causes**:

1. **Hash verification failing** in `store_from_chunks()`
2. **Index commit failing** (but this would be different error)
3. **Some other validation failing** in `commit()`

---

**Status**: ‚è∏Ô∏è **BLOCKED** - Need to add logging to see exact error  
**Completed**: 3/5 fixes (60% - including key derivation alignment)  
**Test Results**: Still 2/5 passing (no improvement)  
**Confidence**: LOW (need error visibility - chunks upload but finish() rejects)  
**Next**: **ADD LOGGING** to see why `finish()` rejects

---

## üö® CRITICAL: We Need Error Visibility

**The test shows "rejected" but NO ERROR MESSAGE**. We're flying blind.

**Immediate Action Needed**:

1. **Add logging to `uploads_finish()`**:

```rust
ic_cdk::println!("FINISH_START: sid={}", session_id);
match upload_service.commit(...) {
    Ok(result) => {
        ic_cdk::println!("FINISH_OK: sid={}", session_id);
        Ok(result)
    }
    Err(e) => {
        ic_cdk::println!("FINISH_ERROR: sid={}, error={:?}", session_id, e);
        Err(e)
    }
}
```

2. **Check logs after test**:

```bash
dfx canister logs backend | grep "FINISH_ERROR"
```

3. **Capture error in test**:

```javascript
try {
    await uploads_finish(...);
} catch (error) {
    console.error("FINISH REJECTED:", error);  // ‚Üê Show actual error!
}
```

---

**Created**: 2025-10-01 22:30  
**Updated**: 2025-10-01 22:45  
**Context**: Fixed 3/5 items but still failing - need error visibility to proceed
