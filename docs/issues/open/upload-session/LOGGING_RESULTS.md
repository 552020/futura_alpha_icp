# Logging Results - Root Cause Found!

**Date**: 2025-10-01 23:45  
**Status**: üéØ **ROOT CAUSE IDENTIFIED** - Hash verification issues revealed  
**Next**: Implement rolling hash to eliminate read-back verification

---

## üîç Test Results With Logging

### Success Pattern

```
[Session 7 - Display 200KB] ‚úÖ SUCCESS
FINISH_START sid=7 expected_len=204800
COMMIT: sid=7 chunks_verified
COMMIT: sid=7 hash_verified blob_id=16406829232824261652
FINISH_INDEX_COMMITTED sid=7 blob=blob_16406829232824261652 mem=mem_1759351376075186000
FINISH_OK sid=7
```

**This proves**:

- ‚úÖ Deterministic keys working
- ‚úÖ Hash verification working
- ‚úÖ Index commit working
- ‚úÖ System architecture is sound!

### Failure Pattern #1: NotFound

```
[Session 8 - Thumb 50KB] ‚ùå NOT FOUND
FINISH_START sid=8 expected_len=51200
COMMIT: sid=8 chunks_verified  ‚Üê Chunks exist check passes
FINISH_ERROR sid=8 err=NotFound  ‚Üê But then blob not found!
```

**Root cause**: Chunks verified but then disappeared when reading back for hash

### Failure Pattern #2: Checksum Mismatch

```
[Session 6 - Placeholder 1KB] ‚ùå CHECKSUM MISMATCH
FINISH_START sid=6 expected_len=1024
COMMIT: sid=6 chunks_verified
FINISH_ERROR sid=6 err=InvalidArgument("checksum_mismatch:
  expected=9b6ce55f379e9771551de6939556a7e6b949814ae27c2f5cfd5dbeb378ce7c2a
  actual  =a223108ae6c7f1693829b0ecaaae2fd252c57585af89bcdd7532a562c10cc6ab")
```

**Root cause**: Chunks being read back have different hash than what client sent

---

## üéØ Diagnosis

### The Problem

**Current flow**:

1. Client sends chunks with SHA256 hash
2. Chunks written to storage via `put_chunk()`
3. `finish()` calls `store_from_chunks()` which **READS CHUNKS BACK** to verify hash
4. **Reading back fails** or **produces wrong hash**

### Why Reading Back Fails

**Two possible causes**:

**A) Race Condition** - Parallel uploads interfering

- Session 7 succeeds (first to finish?)
- Sessions 6 and 8 fail (race with session 7?)
- Chunks might be overwriting each other

**B) Key Collision** - Multiple sessions using same keys

- If `provisional_memory_id` is not unique per upload
- Different sessions could write to same keys
- Last write wins, earlier chunks lost

### Tech Lead's Solution: Rolling Hash

**Instead of reading chunks back**:

```rust
// CURRENT (problematic):
finish() ‚Üí read_all_chunks_from_storage() ‚Üí hash() ‚Üí compare

// ROLLING HASH (correct):
put_chunk() ‚Üí update_rolling_hash() ‚Üí write()
finish() ‚Üí finalize_hash() ‚Üí compare (no read-back!)
```

**Benefits**:

- ‚úÖ No read-back needed
- ‚úÖ Hash updated during write (faster)
- ‚úÖ No race conditions from reading
- ‚úÖ Immediate hash verification

---

## üìä Success Rate Analysis

| Session | Type        | Size  | Result      | Issue                   |
| ------- | ----------- | ----- | ----------- | ----------------------- |
| 7       | Display     | 200KB | ‚úÖ Success  | None!                   |
| 8       | Thumb       | 50KB  | ‚ùå NotFound | Chunks disappeared      |
| 6       | Placeholder | 1KB   | ‚ùå Checksum | Wrong hash on read-back |

**Success Rate**: 1/3 (33%) for parallel uploads  
**But**: 1 success proves architecture works!

---

## üöÄ Next Step: Implement Rolling Hash

### Implementation (30 minutes)

```rust
// 1. Add thread-local hash storage
thread_local! {
    static UPLOAD_HASH: RefCell<BTreeMap<u64, Sha256>> = RefCell::new(BTreeMap::new());
}

// 2. Initialize in uploads_begin()
UPLOAD_HASH.with(|m| { m.borrow_mut().insert(sid, Sha256::new()); });

// 3. Update in uploads_put_chunk()
UPLOAD_HASH.with(|m| {
    m.borrow_mut().get_mut(&sid)?.update(&bytes);
});

// 4. Verify in uploads_finish()
let computed = UPLOAD_HASH.with(|m| {
    m.borrow_mut().remove(&sid)?.finalize().to_vec()
});
if computed != expected_sha256 {
    return Err(ChecksumMismatch);
}
// Skip store_from_chunks hash verification!
```

### Changes Needed

1. **`src/backend/src/lib.rs`** - Add UPLOAD_HASH thread-local
2. **`src/backend/src/lib.rs::uploads_begin()`** - Initialize hash
3. **`src/backend/src/lib.rs::uploads_put_chunk()`** - Update hash
4. **`src/backend/src/lib.rs::uploads_finish()`** - Verify hash
5. **`src/backend/src/upload/blob_store.rs::store_from_chunks()`** - Remove hash verification (keep length check)

---

## üí° Key Insights

### What Works ‚úÖ

1. **Deterministic SHA256 keys** - Session 7 proves this works
2. **pmid_hash32()** function - Consistent key derivation
3. **Index commit** - Memory inserted and retrievable
4. **Logging** - Pinpoints exact failure points

### What's Broken ‚ùå

1. **Read-back hash verification** - Unreliable in parallel mode
2. **No rolling hash** - Must read chunks back (slow + buggy)

### Why Session 7 Succeeded

**Hypothesis**: First to finish, so no race condition  
**Or**: Larger file (200KB) vs smaller files (50KB, 1KB) ‚Üí timing difference

---

## üéØ Expected Outcome After Rolling Hash

**Current**: 1/3 parallel uploads succeed (33%)  
**After rolling hash**: 3/3 parallel uploads succeed (100%)

**Why**:

- No more read-back (eliminates race)
- Hash computed during write (correct data)
- Faster (no extra read pass)

---

## üìã Implementation Checklist

- [ ] Add `UPLOAD_HASH` thread-local in lib.rs
- [ ] Initialize hash in `uploads_begin()`
- [ ] Update hash in `uploads_put_chunk()`
- [ ] Verify hash in `uploads_finish()`
- [ ] Remove hash verification from `store_from_chunks()`
- [ ] Keep length verification in `store_from_chunks()`
- [ ] Test parallel uploads
- [ ] Verify all 5 tests pass

**Estimated time**: 30-45 minutes

---

**Status**: üü¢ **READY TO IMPLEMENT** - Clear path to fix  
**Confidence**: VERY HIGH - Logging shows exact issue  
**Blocker**: None - know exactly what to do  
**Next**: Implement rolling hash

---

**Created**: 2025-10-01 23:45  
**Root Cause**: Read-back hash verification unreliable  
**Solution**: Rolling hash (update during put_chunk)  
**ETA to working**: 30-45 minutes
